
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AD-PT</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <link rel="icon" type="image/png" href="../img/newyork.ico">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link href="css/twentytwenty.css" rel="stylesheet">
    <link href="css/foundation.css" rel="stylesheet">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Theme Stylesheets -->
    <!-- <link href="css/theme.css" rel="stylesheet"> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <!-- CityNeRF: Building NeRF at City Scale -->
                <strong>[NeurIPS 2023] AD-PT: Autonomous Driving Pre-Training <br> Large-scale Point Cloud Dataset</strong>
                <!-- <small>CVPR 2023</small> -->
            </h1>
            <!-- <a style="text-align:center" href="https://pjlab-adg.github.io/">ADLab at Shanghai AI Laboratory</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <div style="margin-bottom: 0.7em; margin-top:0.2em" class="authors">
                    <a style="color:#000000;" href="https://jiakangyuan.github.io/">Jiakang Yuan<sup>1,2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://bobrown.github.io/boZhang.github.io/">Bo Zhang<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://sky-fly97.github.io/">Xiangchao Yan<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://eetchen.github.io/">Tao Chen<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://scholar.google.com.hk/citations?user=K0PpvLkAAAAJ&hl=zh-CN">Botian Shi<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://liyikang.top/">Yikang Li<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                    <a style="color:#000000;" href="https://scholar.google.com.hk/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;
                </div>

                <div style="margin-bottom: 0.5em;" class="affiliations">
                    <a href="http://fudan.edu.cn/">Fudan University<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <a href="https://pjlab-adg.github.io/">ADLab at Shanghai AI Laboratory<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                </div>
<!-- 
                <div style="margin-bottom: 0.7em;" class="col-md-12 text-center">
                    *denotes correspondant authors
                </div> -->

            </div>
        </div>

        <div style="margin-bottom: 0.7em;" class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2306.00612">
                            <image src="./img/paper.png" height="50px"><br>
                                <h5><strong>AD-PT Paper</strong></h5>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/PJLab-ADG/3DTrans">
                            <image src="./img/github_pad.png" height="50px"><br>
                                <h5><strong>AD-PT Code</strong></h5>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <br>
                <!-- <image src="img/Uni3D-teaser" class="img-responsive" alt="overview"><br> -->
                <p class="text-justify">
                    It is a long-term vision for Autonomous Driving (AD) community that the perception models can learn from a large-scale point cloud dataset, to obtain unified representations that can achieve promising results on different tasks or benchmarks. Previous works mainly focus on the self-supervised pre-training pipeline, meaning that they perform the pre-training and fine-tuning on the same benchmark, which is difficult to attain the performance scalability and cross-dataset application for the pre-training checkpoint. In this paper, for the first time, we are committed to building a large-scale pre-training point-cloud dataset with diverse data distribution, and meanwhile learning generalizable representations from such a diverse pre-training dataset. We formulate the point-cloud pre-training task as a semi-supervised problem, which leverages the few-shot labeled and massive unlabeled point-cloud data to generate the unified backbone representations that can be directly applied to many baseline models and benchmarks, decoupling the AD-related pre-training process and downstream fine-tuning task. During the period of backbone pre-training, by enhancing the scene- and instance-level distribution diversity and exploiting the backbone's ability to learn from unknown instances, we achieve significant performance gains on a series of downstream perception benchmarks including Waymo, nuScenes, and KITTI, under different baseline models like PV-RCNN++, SECOND, CenterPoint.             
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Drawback of previous 3D pre-training methods
                </h3>
                <image src="img/AD-PT-0.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify", style="margin-top: 10px;">
                    <strong> Differences between previous pre-training paradigm and the proposed AD-PT paradigm.
                    </strong>
                    <!-- (
                        <a style="color:#000000;" href="https://www.google.com/help/terms_maps/">
                        ©2021 Google
                        </a>
                    ) -->
                </p>
                <h3>
                    Framework
                </h3>
                <image src="img/AD-PT.png" class="img-responsive" alt="overview"><br>
                <p class="small">
                    <strong> The overview of the proposed AD-PT. By leveraging the proposed method to train on the unified large-scale point cloud dataset, we can obtain well-generalized pre-training parameters that can be applied to multiple datasets and support different baseline detectors. 
                    </strong>
                    <!-- (
                        <a style="color:#000000;" href="https://www.google.com/help/terms_maps/">
                        ©2021 Google
                        </a>
                        ) -->
                </p>
                <h3>
                    Data preparation and statistical distribution using re-scaling
                </h3>
                <image src="img/AD-PT-data.png" class="img-responsive" alt="overview"><br>
                <p class="small">
                    <strong> Overall dataset preparation procedure.
                    </strong>
                </p>
                <h3>
                    Visualization of re-sampling
                </h3>
                <image src="img/AD-PT-sampling.png" class="img-responsive" alt="overview"><br>
                <p class="small">
                    <strong> Visualization of point-to-beam playback re-sampling.
                    </strong>
                </p>
                <h3>
                    Results on Waymo dataset
                </h3>
                <image src="img/result-1.png" class="img-responsive" alt="overview"><br>
                <h3>
                    Results on nuScenes dataset
                </h3>
                <image src="img/result-2.png" class="img-responsive" alt="overview"><br>
                
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bibtex
                </h3>
                <br>
                <!-- <image src="img/Uni3D-teaser" class="img-responsive" alt="overview"><br> -->
                <pre>
                    <code>
@inproceedings{yuan2023ad-pt,
    title={AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset},
    author={Yuan, Jiakang and Zhang, Bo and Yan, Xiangchao and Chen, Tao and Shi, Botian and Li, Yikang and Qiao, Yu},
    booktitle={Advances in Neural Information Processing Systems},
    year={2023}
}
                    </code>
                </pre>
                    
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
</textarea>
                </div>
            </div>
        </div> -->
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div> -->
    </div>
</body>

	<script type="text/javascript">
        var slideIndex = 1;
        showSlides(slideIndex);

        // Next/previous controls
        function plusSlides(n) {
        showSlides(slideIndex += n);
        }

        // Thumbnail image controls
        function currentSlide(n) {
        showSlides(slideIndex = n);
        }

        function showSlides(n) {
        var i;
        var slides = document.getElementsByClassName("mySlides");
        var dots = document.getElementsByClassName("dot");
        if (n > slides.length) {slideIndex = 1}
        if (n < 1) {slideIndex = slides.length}
        for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
        }
        for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
        }
        slides[slideIndex-1].style.display = "block";
        dots[slideIndex-1].className += " active";
        }
	</script>



    <!-- Image Slider Javascripts -->
    <script src="js/jquery.event.move.js"></script>
    <script src="js/jquery.twentytwenty.js"></script>
    <script>
        $(window).on('load',function() {
            $("#images").twentytwenty();
        });
    </script>
    <script>
        $(function(){
            $(".twentytwenty-container[data-orientation!='vertical']").twentytwenty({default_offset_pct: 0.49, before_label: 'Before', after_label: 'After'});
        });
    </script>
</html>
